---
title: "HW1"
author: "Elle Boodsakorn jb79226"
date: "02/11/2022"
output: md_document
---

```{r}

library(readr)
library(dplyr)
library(expss)
library(ggplot2)
library(tidyverse)
library(data.table)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)

```

* 1.

```{r}
ABIA <- read_csv("/Users/jirapat/Desktop/R/Data Mining/ABIA.csv")
ABIA$DepartureHour <- round(ABIA$DepTime/100, 0)

ABIA <- ABIA %>% mutate(Month_label = ifelse(ABIA$Month %in% c(12, 1, 2), "Winter", ifelse(ABIA$Month %in% c(3, 4, 5), "Spring", ifelse(ABIA$Month %in% c(6, 7, 8), "Summer", "Fall"))))

```

* What is the best time of day to fly to minimize delays
The best time of the day to fly to minimize delays is 5am., 6am., and 7am. respectively.

```{r}
ABIA %>% group_by(DepartureHour) %>% summarise(mean_dep_delay = mean(DepDelay, na.rm=TRUE)) %>% arrange(mean_dep_delay)
```

* Does this change by airline?
for most airlines, such as 9E (Endeavor Air), B6 (JetBlue), CO (Copa Airline), DL (Delta Airline), and WN (SouthWest Airline) show least delay between 5am., 6am., and 7am.

```{r}
mean_delay_by_hours <- ABIA %>% group_by(DepartureHour, UniqueCarrier) %>% summarise(mean_dep_delay = mean(DepDelay, na.rm=TRUE)) %>% arrange(mean_dep_delay)

mean_delay_by_hours

ggplot(mean_delay_by_hours) + geom_col(aes(x=DepartureHour, y=mean_dep_delay)) + facet_wrap(~UniqueCarrier) + labs(title = "Mean Departure Delay Each Hour by Airline") + xlab("Departure Time") + ylab("Mean Departure Delay")

```

* What is the best time of year to fly to minimize delays
For simplicity, let's group the months together by seasons for the ease of analysis. The bar graph shows that average departure delay is highest in Spring and Winter which makes sense intuitively since those are holiday seasons. In contrast, the average departure delay is least in Fall since it is when school is starting and so most people are not flying.

```{r}
mean_dep_delay_by_season <- ABIA %>% group_by(Month_label, Dest) %>% summarise(mean_dep_delay = mean(DepDelay, na.rm=TRUE)) %>% arrange(mean_dep_delay) 

mean_dep_delay_by_season

ggplot(mean_dep_delay_by_season, aes(Month_label, mean_dep_delay)) + geom_col()

```

* Does this change by destination? (You'd probably want to focus on a handful of popular destinations.)
By grouping flying activities by destinations and sorting the data we observed that the top 6 most popular destinations are DAL (Dallas), DFW (Dallas), IAH (Houston), PHX (Pheonix), DEN (Denver), and ORD (Illinois). The bar graph shows high average departure delay in Spring especially for Dallas and Illinois. All of the 6 destinations commonly show least average departure delay in Fall.

```{r}
ABIA %>% group_by(Dest) %>% summarise(count = n()) %>% arrange(desc(count)) 

top_dest <- c("DAL", "DFW", "IAH", "PHX", "DEN", "ORD")

mean_dep_delay_by_season_top_dest <- mean_dep_delay_by_season %>% filter(Dest %in% top_dest)

ggplot(mean_dep_delay_by_season_top_dest, aes(Month_label, mean_dep_delay)) + geom_col() + facet_wrap(~Dest) + labs(title = "Mean Departure Delay Each Season by Top Destination") + xlab("Seasons") + ylab("Mean Departure Delay")

```

* 2A.
Caption: The top 10 most popular song since 1958 as measured by the total number of weeks that a song spent on the Billboard Top 100.

Analysis: From the table, we see that Radioactive by Imagine Dragons is highly popular. It stayed on Billboard Top 100 for 87 weeks. Sail, Blinding Lights, and I'm Yours follow closely with the total number of weeks spent on the Billboard top 100 equal to 79, 76, and 76 respectively.

```{r}
billboard <- read_csv("/Users/jirapat/Desktop/R/Data Mining/billboard.csv")

top_10 <- billboard %>% group_by(song, performer) %>% summarize(count = n()) %>% arrange(desc(count)) %>% head(10)
top_10 <- apply_labels(top_10, song = "Song", count = "Total Number of Weeks", performer = "Performer")

```

* 2B.
Caption: Line graph showing number of unique songs each year from 1959 to 2020.

Analysis: We see an increasing trend in musical diversity (as measured by number of unique songs) during 1960 and reached its peak at 1965. After that number of unique songs declined drastically and reached its lowest value of 400 in the year 2000. Then, the trend rose exponentially again until 2020.

```{r}
billboard_b <- billboard %>% filter(!year %in% c("1958", "2021")) %>% group_by(year) %>% mutate(unique_songs = n_distinct(song))

billboard_diversity <- billboard_b %>% distinct(year, .keep_all = TRUE)

ggplot(billboard_diversity, aes(x=year, y=unique_songs)) + geom_line() + xlab("year")

```

* 2C.
Caption: The horizontal bar graph displays total number of song appeared on ten weeks hit for each 19 artists.

Analysis: Ten weeks hit is for the songs that appear on Billboard Top 100 for at least 10 weeks. There are 19 artists with at least 30 songs on ten weeks hit. For example, Elton John had 52 songs on ten week hit, meaning, each of the 52 songs was featured on Billboard Top 100 for at least 10 weeks. Madonna had 44 songs on ten weeks hit followed by Kenny Chesney who had 42  songs.

```{r}
ten_week_hit <- billboard %>% group_by(song, performer) %>% summarize(count = n()) %>% arrange(desc(count)) %>% filter(count >= 10) %>% group_by(performer) %>% count() %>% filter(n >= 30)

ggplot(ten_week_hit, aes(x = performer, y = n)) + geom_col() + coord_flip()

```

* 3A.
The 95th percentile of heights for female across all Athletics events is 183 cm. This means that 95% of the height for female across all Athletics events is 183 cm. or lower.

```{r}
olympics_top20 <- read_csv("/Users/jirapat/Desktop/R/Data Mining/olympics_top20.csv") 

unique(olympics_top20$sport)

olympics_top20_parta <- olympics_top20 %>% filter(sex == "F" & sport == "Athletics")
quantile(olympics_top20_parta$height, probs = 0.95)
```

* 3B.
Rowing Women's Coxed Fours had the greatest variability in competitor's height as measured by standard deviation of 10.9.

```{r}
olympics_top20 %>% filter(sex == "F") %>% group_by(event) %>% summarise(sd_height = sd(height)) %>% arrange(desc(sd_height)) %>% slice(c(1))

```

* 3C.

* How has the average age of Olympic swimmers changed over time?
The trend for average age of Olympic swimmers fluctuated a lot since 1900. The average age rose from 18 in 1900 to 27 in 1912. Then the trend constantly declined and reached its low point at 18.5 in 1976. After that, the trend took off and started to rise at a decreasing rate until 2016.

```{r}
olympics_top20 %>% filter(sport == "Swimming") %>% group_by(year) %>% summarise(mean_age_swim = mean(age)) %>% ggplot(aes(x = year, y = mean_age_swim)) + geom_line()
```

* How has the average age of Olympic swimmers changed over time for Male?
The trend of the average age for Olympic male swimmers reached its peak in the year 1924 with the value of 32 years old. Then the average age dropped drastically and reached its minimum in the year 1932 with the value of 19 years old. After that, the trend has been rising slowly and reached the average age of 24.13 in the year 2016.

```{r}
olympics_top20_male <- olympics_top20 %>% filter(sport == "Swimming") %>% filter(sex == "M") %>% group_by(year) %>% summarise(mean_age_swim = mean(age)) %>% ggplot(aes(x = year, y = mean_age_swim)) + geom_line()

olympics_top20_male
```

* How has the average age of Olympic swimmers changed over time for Female?
The trend for women, on the other hand, remained low until 1975. Then it rose drastically to reach its peak value of 22.5 in 2000. After that, the average ages had been more or less constant at around 22 years old.

```{r}
olympics_top20_female <- olympics_top20 %>% filter(sport == "Swimming") %>% filter(sex == "F") %>% group_by(year) %>% summarise(mean_age_swim = mean(age)) %>% ggplot(aes(x = year, y = mean_age_swim)) + geom_line()

olympics_top20_female
```

* Does the trend look different for male swimmers relative to female swimmers?
The trends for male and female differed substantially and seemed to show no correlation. The average age for male reached its peak in the year 1924 with the value of 32 years old. While the average ages for women was highest in 2000 with the value of 22.5. Overall, the trend of average ages for men declined over time. On the contrary, the trend rose over time for women

```{r}
olympics_top20 %>% filter(sport == "Swimming") %>% group_by(sex, year) %>% summarise(mean_age = mean(age)) %>% ggplot(aes(x = year, y = mean_age, colour = sex)) + geom_line()
```

* 4

```{r}
sclass <- read_csv("/Users/jirapat/Desktop/R/Data Mining/sclass.R")
mercedes = sclass

mercedes_350 = mercedes %>% filter(trim == '350')

summary(mercedes_350)

# plot the data
ggplot(data = mercedes_350) + 
  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey')

# test train split
mercedes_350_split = initial_split(mercedes_350, prop=0.8)
mercedes_350_train = training(mercedes_350_split)
mercedes_350_test  = testing(mercedes_350_split)

k_grid = c(2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45,
           50, 60, 70, 80, 90, 100)

k_grid = seq(2,100, by=1)

cv_350_grid = foreach(k = k_grid, .combine='rbind') %dopar% {
  knn = knnreg(price ~ mileage, data=mercedes_350_train, k=k)
  rms = rmse(knn, mercedes_350_test)
  c(k=k, err=rms)
} %>% as.data.frame

head(cv_350_grid)

ggplot(cv_350_grid) + 
  geom_point(aes(x=k, y=err))
  scale_x_log10()

cv_grid_output = cv_350_grid %>% filter(err == min(cv_350_grid$err))
cv_grid_output$k

knn = knnreg(price ~ mileage, data=mercedes_350_train, k=cv_grid_output$k)

mercedes_350_test = mercedes_350_test %>%
  mutate(price_350_pred = predict(knn, mercedes_350_test))

pred_350_test = ggplot(data = mercedes_350_test) + 
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2)
pred_350_test

pred_350_test + geom_line(aes(x = mileage, y = price_350_pred), color='red', size=1.5)

```



```{r}
mercedes_65 = mercedes %>% filter(trim == '65 AMG')

summary(mercedes_65)

ggplot(data = mercedes_65) + 
  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey')

mercedes_65_split = initial_split(mercedes_65, prop=0.8)
mercedes_65_train = training(mercedes_65_split)
mercedes_65_test  = testing(mercedes_65_split)

k_grid = c(2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45,
           50, 60, 70, 80, 90, 100)

k_grid = seq(2,100, by=1)

cv_65_grid = foreach(k = k_grid, .combine='rbind') %dopar% {
  knn = knnreg(price ~ mileage, data=mercedes_65_train, k=k)
  rms = rmse(knn, mercedes_65_test)
  c(k=k, err=rms)
} %>% as.data.frame

head(cv_65_grid)

ggplot(cv_65_grid) + 
  geom_point(aes(x=k, y=err))
  scale_x_log10()

cv_grid_output = cv_65_grid %>% filter(err == min(cv_65_grid$err))
cv_grid_output$k

knn = knnreg(price ~ mileage, data=mercedes_65_train, k=cv_grid_output$k)

mercedes_65_test = mercedes_65_test %>%
  mutate(price_65_pred = predict(knn, mercedes_65_test))

pred_65_test = ggplot(data = mercedes_65_test) + 
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2)
pred_65_test

pred_65_test + geom_line(aes(x = mileage, y = price_65_pred), color='red', size=1.5)

```
* Which trim yields a larger optimal value of K? -- trim = 350 yields a larger optimal k

Why do you think this is?
The sample size for trim = 350 is higher than that for trim = 65. If we have larger sample size, we can afford higher k without the bias being too high. This is because you're averaging the points around the neighbourhood. This is about bias-variance tradeoff. On the contrary, if you have small sample size, you're averaging the data points further away, causing the bias to be high.






